{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPAhgQrSISZtB3g2PzW5Le",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Park-New-project/2022_AI_Study_Course/blob/main/%5BAI%5D_221101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49et6O3_Ql1O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "471efa50-1977-4e3c-e8aa-06f429277eb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.6.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mP9V7FsKzInH",
        "outputId": "fb7da6fb-ea91-4628-8367-52a2fc3bfb47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.6.1\n",
            "  Downloading tensorflow-2.6.1-cp37-cp37m-manylinux2010_x86_64.whl (458.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 458.3 MB 11 kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.1) (2.9.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.1) (3.19.6)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.1) (0.38.3)\n",
            "Collecting typing-extensions~=3.7.4\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.1) (2.9.1)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.1) (3.3.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.1) (1.50.0)\n",
            "Collecting absl-py~=0.10\n",
            "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 60.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.1) (0.2.0)\n",
            "Collecting clang~=5.0\n",
            "  Downloading clang-5.0.tar.gz (30 kB)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.1) (0.4.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.1) (1.12)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.1) (1.15.0)\n",
            "Collecting numpy~=1.19.2\n",
            "  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.8 MB 40.3 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.7\n",
            "  Downloading tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 50.6 MB/s \n",
            "\u001b[?25hCollecting termcolor~=1.1.0\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "Collecting wrapt~=1.12.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.1) (1.1.2)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.1) (3.1.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.6.1) (1.6.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow==2.6.1) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.6.1) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.6.1) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.6.1) (2.14.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.6.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.6.1) (3.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.6.1) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.6.1) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.6.1) (1.8.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.6.1) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.6.1) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.6.1) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.6.1) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.6.1) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.6.1) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.6.1) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6.1) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.6.1) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.6.1) (3.2.2)\n",
            "Building wheels for collected packages: clang, termcolor, wrapt\n",
            "  Building wheel for clang (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30694 sha256=37893081e15c52a7fbb1270b4e86d7459f51bbc15ebe224ca7d8bac2f06c2bc9\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/91/04/971b4c587cf47ae952b108949b46926f426c02832d120a082a\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4849 sha256=2e81a96c80199fa0f0b17bb49ec3eb7614600ff95d9ab20a093dd91aa9b2b40c\n",
            "  Stored in directory: /root/.cache/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68717 sha256=f2532756713fd13747d9bc8733f4a55ba6d4e24a92f7e5695281c553462fa750\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
            "Successfully built clang termcolor wrapt\n",
            "Installing collected packages: typing-extensions, numpy, absl-py, wrapt, termcolor, tensorflow-estimator, clang, tensorflow\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.1.1\n",
            "    Uninstalling typing-extensions-4.1.1:\n",
            "      Successfully uninstalled typing-extensions-4.1.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.6\n",
            "    Uninstalling numpy-1.21.6:\n",
            "      Successfully uninstalled numpy-1.21.6\n",
            "  Attempting uninstall: absl-py\n",
            "    Found existing installation: absl-py 1.3.0\n",
            "    Uninstalling absl-py-1.3.0:\n",
            "      Successfully uninstalled absl-py-1.3.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.14.1\n",
            "    Uninstalling wrapt-1.14.1:\n",
            "      Successfully uninstalled wrapt-1.14.1\n",
            "  Attempting uninstall: termcolor\n",
            "    Found existing installation: termcolor 2.1.0\n",
            "    Uninstalling termcolor-2.1.0:\n",
            "      Successfully uninstalled termcolor-2.1.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n",
            "pydantic 1.10.2 requires typing-extensions>=4.1.0, but you have typing-extensions 3.7.4.3 which is incompatible.\n",
            "jaxlib 0.3.22+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "jax 0.3.23 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
            "cmdstanpy 1.0.8 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\u001b[0m\n",
            "Successfully installed absl-py-0.15.0 clang-5.0 numpy-1.19.5 tensorflow-2.6.1 tensorflow-estimator-2.6.0 termcolor-1.1.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "typing_extensions"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras==2.6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGkqdxpQzKSv",
        "outputId": "d8ac06a9-617e-45d0-f198-a61748db3c91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras==2.6\n",
            "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 8.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: keras\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "Successfully installed keras-2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "C_WAnjBsyjDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df= pd.read_csv(\"/content/gdrive/MyDrive/Colab/data/housing.csv\",\n",
        "                delim_whitespace=True, header=None) # 공백으로 구분\n",
        "\n",
        "data = df.values\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Dense(30, input_dim=13, activation='relu'))\n",
        "model.add(Dense(6, activation='relu'))\n",
        "model.add(Dense(1)) #ws\n",
        "\n",
        "model.summary() \n",
        "model.compile( loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "model.fit(x_train, y_train, epochs=200, batch_size=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgyjkDb-yqt4",
        "outputId": "d31eba4b-786a-4cb7-b5bf-eb493790784f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 30)                420       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 6)                 186       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 7         \n",
            "=================================================================\n",
            "Total params: 613\n",
            "Trainable params: 613\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "36/36 [==============================] - 1s 3ms/step - loss: 1026.5002\n",
            "Epoch 2/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 124.4267\n",
            "Epoch 3/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 88.8475\n",
            "Epoch 4/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 85.2456\n",
            "Epoch 5/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 80.9225\n",
            "Epoch 6/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 77.8915\n",
            "Epoch 7/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 77.4725\n",
            "Epoch 8/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 74.3579\n",
            "Epoch 9/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 71.9138\n",
            "Epoch 10/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 68.7956\n",
            "Epoch 11/200\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 69.7647\n",
            "Epoch 12/200\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 65.6983\n",
            "Epoch 13/200\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 65.1808\n",
            "Epoch 14/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 62.5616\n",
            "Epoch 15/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 60.6548\n",
            "Epoch 16/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 59.0778\n",
            "Epoch 17/200\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 56.9367\n",
            "Epoch 18/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 57.7117\n",
            "Epoch 19/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 55.5268\n",
            "Epoch 20/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 57.4490\n",
            "Epoch 21/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 55.2652\n",
            "Epoch 22/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 51.7715\n",
            "Epoch 23/200\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 51.1689\n",
            "Epoch 24/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 51.5728\n",
            "Epoch 25/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 49.6172\n",
            "Epoch 26/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 48.0706\n",
            "Epoch 27/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 46.4707\n",
            "Epoch 28/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 51.1396\n",
            "Epoch 29/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 47.6478\n",
            "Epoch 30/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 43.6078\n",
            "Epoch 31/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 45.9805\n",
            "Epoch 32/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 52.5179\n",
            "Epoch 33/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 42.2738\n",
            "Epoch 34/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 45.9657\n",
            "Epoch 35/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 42.9305\n",
            "Epoch 36/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 42.2577\n",
            "Epoch 37/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 41.8079\n",
            "Epoch 38/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 48.1219\n",
            "Epoch 39/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 42.8513\n",
            "Epoch 40/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 41.6502\n",
            "Epoch 41/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 39.2726\n",
            "Epoch 42/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 39.0822\n",
            "Epoch 43/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 39.3225\n",
            "Epoch 44/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 41.1399\n",
            "Epoch 45/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 38.3490\n",
            "Epoch 46/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 37.5721\n",
            "Epoch 47/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 37.1382\n",
            "Epoch 48/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 37.8716\n",
            "Epoch 49/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 38.3074\n",
            "Epoch 50/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 36.1690\n",
            "Epoch 51/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 36.5544\n",
            "Epoch 52/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 43.9514\n",
            "Epoch 53/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 36.8434\n",
            "Epoch 54/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 35.0238\n",
            "Epoch 55/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 36.8112\n",
            "Epoch 56/200\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 36.9333\n",
            "Epoch 57/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 35.1777\n",
            "Epoch 58/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 35.4911\n",
            "Epoch 59/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 35.1500\n",
            "Epoch 60/200\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 34.9393\n",
            "Epoch 61/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 35.1089\n",
            "Epoch 62/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 36.1626\n",
            "Epoch 63/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 33.6123\n",
            "Epoch 64/200\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 34.9464\n",
            "Epoch 65/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 33.9821\n",
            "Epoch 66/200\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 41.8601\n",
            "Epoch 67/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 35.6391\n",
            "Epoch 68/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 34.9008\n",
            "Epoch 69/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 33.9582\n",
            "Epoch 70/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 37.3325\n",
            "Epoch 71/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 34.9285\n",
            "Epoch 72/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 33.9959\n",
            "Epoch 73/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 34.7493\n",
            "Epoch 74/200\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 37.5241\n",
            "Epoch 75/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 35.7620\n",
            "Epoch 76/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 33.1572\n",
            "Epoch 77/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 33.3441\n",
            "Epoch 78/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 32.7257\n",
            "Epoch 79/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 32.5426\n",
            "Epoch 80/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 34.6344\n",
            "Epoch 81/200\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 35.8175\n",
            "Epoch 82/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 33.7094\n",
            "Epoch 83/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 34.4640\n",
            "Epoch 84/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 33.8430\n",
            "Epoch 85/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 34.4935\n",
            "Epoch 86/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 33.6482\n",
            "Epoch 87/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 35.0582\n",
            "Epoch 88/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 33.9852\n",
            "Epoch 89/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 33.8099\n",
            "Epoch 90/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 32.2337\n",
            "Epoch 91/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 31.7769\n",
            "Epoch 92/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 33.1034\n",
            "Epoch 93/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 35.0965\n",
            "Epoch 94/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 31.2891\n",
            "Epoch 95/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 31.0203\n",
            "Epoch 96/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 33.4854\n",
            "Epoch 97/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 32.3903\n",
            "Epoch 98/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 30.7583\n",
            "Epoch 99/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 33.5057\n",
            "Epoch 100/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 30.9902\n",
            "Epoch 101/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 32.0646\n",
            "Epoch 102/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 30.0175\n",
            "Epoch 103/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 31.9311\n",
            "Epoch 104/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 29.6897\n",
            "Epoch 105/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 30.7588\n",
            "Epoch 106/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 32.0274\n",
            "Epoch 107/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 31.0466\n",
            "Epoch 108/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 37.6013\n",
            "Epoch 109/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 30.3919\n",
            "Epoch 110/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 30.7547\n",
            "Epoch 111/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 29.2457\n",
            "Epoch 112/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 33.6121\n",
            "Epoch 113/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 32.9829\n",
            "Epoch 114/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 30.9544\n",
            "Epoch 115/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 29.8898\n",
            "Epoch 116/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 30.1112\n",
            "Epoch 117/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 29.0347\n",
            "Epoch 118/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 30.0853\n",
            "Epoch 119/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 34.0072\n",
            "Epoch 120/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 30.8721\n",
            "Epoch 121/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 33.9696\n",
            "Epoch 122/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 29.7274\n",
            "Epoch 123/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 31.5237\n",
            "Epoch 124/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 29.5577\n",
            "Epoch 125/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 28.7125\n",
            "Epoch 126/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 30.5145\n",
            "Epoch 127/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 31.2760\n",
            "Epoch 128/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 29.4734\n",
            "Epoch 129/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.8636\n",
            "Epoch 130/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.4343\n",
            "Epoch 131/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 31.6378\n",
            "Epoch 132/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.5273\n",
            "Epoch 133/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 30.8949\n",
            "Epoch 134/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 27.7699\n",
            "Epoch 135/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 32.1860\n",
            "Epoch 136/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 31.4251\n",
            "Epoch 137/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 29.7767\n",
            "Epoch 138/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 34.1585\n",
            "Epoch 139/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 31.7846\n",
            "Epoch 140/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 31.2364\n",
            "Epoch 141/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 29.4039\n",
            "Epoch 142/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 29.5231\n",
            "Epoch 143/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 30.3576\n",
            "Epoch 144/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 33.9457\n",
            "Epoch 145/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 33.7841\n",
            "Epoch 146/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 35.1460\n",
            "Epoch 147/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 36.1350\n",
            "Epoch 148/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.9044\n",
            "Epoch 149/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 39.2349\n",
            "Epoch 150/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 36.0379\n",
            "Epoch 151/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 28.1169\n",
            "Epoch 152/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.4683\n",
            "Epoch 153/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 29.6715\n",
            "Epoch 154/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 32.2198\n",
            "Epoch 155/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 31.0106\n",
            "Epoch 156/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.2272\n",
            "Epoch 157/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 31.1327\n",
            "Epoch 158/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.0731\n",
            "Epoch 159/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.7566\n",
            "Epoch 160/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.4017\n",
            "Epoch 161/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.7415\n",
            "Epoch 162/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.0574\n",
            "Epoch 163/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.5398\n",
            "Epoch 164/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 33.6420\n",
            "Epoch 165/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 26.7954\n",
            "Epoch 166/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.0060\n",
            "Epoch 167/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 28.1636\n",
            "Epoch 168/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 32.3498\n",
            "Epoch 169/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.1743\n",
            "Epoch 170/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 26.4748\n",
            "Epoch 171/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 32.4039\n",
            "Epoch 172/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.3779\n",
            "Epoch 173/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 29.6861\n",
            "Epoch 174/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.5322\n",
            "Epoch 175/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 29.1974\n",
            "Epoch 176/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 32.8606\n",
            "Epoch 177/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.9523\n",
            "Epoch 178/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.6232\n",
            "Epoch 179/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 26.2644\n",
            "Epoch 180/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.9960\n",
            "Epoch 181/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 27.0524\n",
            "Epoch 182/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.5980\n",
            "Epoch 183/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 28.4806\n",
            "Epoch 184/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.8134\n",
            "Epoch 185/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 31.9844\n",
            "Epoch 186/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 29.8390\n",
            "Epoch 187/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.7633\n",
            "Epoch 188/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.4584\n",
            "Epoch 189/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 31.6660\n",
            "Epoch 190/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.4780\n",
            "Epoch 191/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 28.2803\n",
            "Epoch 192/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.1062\n",
            "Epoch 193/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.9182\n",
            "Epoch 194/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 26.9624\n",
            "Epoch 195/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 29.3661\n",
            "Epoch 196/200\n",
            "36/36 [==============================] - 0s 1ms/step - loss: 33.2010\n",
            "Epoch 197/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.0807\n",
            "Epoch 198/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.1769\n",
            "Epoch 199/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 30.0598\n",
            "Epoch 200/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.9829\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1d0b32c110>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측값과 실제값 비교\n",
        "y_pred = model.predict(x_test)\n",
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG8MX7uAyrto",
        "outputId": "fd3f24a4-dc96-4f75-dbac-962618a14d60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[25.855886 ],\n",
              "       [13.758203 ],\n",
              "       [22.148754 ],\n",
              "       [19.378578 ],\n",
              "       [30.665382 ],\n",
              "       [38.675354 ],\n",
              "       [27.451576 ],\n",
              "       [16.445278 ],\n",
              "       [14.68857  ],\n",
              "       [34.373184 ],\n",
              "       [35.73653  ],\n",
              "       [25.104418 ],\n",
              "       [30.234303 ],\n",
              "       [30.945295 ],\n",
              "       [19.967203 ],\n",
              "       [30.364902 ],\n",
              "       [24.677885 ],\n",
              "       [23.374546 ],\n",
              "       [22.192947 ],\n",
              "       [36.818386 ],\n",
              "       [21.346704 ],\n",
              "       [13.232263 ],\n",
              "       [21.20731  ],\n",
              "       [20.163645 ],\n",
              "       [27.855843 ],\n",
              "       [30.907291 ],\n",
              "       [18.372423 ],\n",
              "       [35.17461  ],\n",
              "       [20.851646 ],\n",
              "       [37.17119  ],\n",
              "       [29.816927 ],\n",
              "       [17.058245 ],\n",
              "       [18.290375 ],\n",
              "       [17.927189 ],\n",
              "       [24.93911  ],\n",
              "       [14.475602 ],\n",
              "       [25.680397 ],\n",
              "       [19.178524 ],\n",
              "       [30.780167 ],\n",
              "       [13.193921 ],\n",
              "       [42.16373  ],\n",
              "       [28.398947 ],\n",
              "       [22.952267 ],\n",
              "       [24.57362  ],\n",
              "       [14.581895 ],\n",
              "       [20.039904 ],\n",
              "       [26.60867  ],\n",
              "       [30.541538 ],\n",
              "       [23.104023 ],\n",
              "       [29.032475 ],\n",
              "       [30.646107 ],\n",
              "       [26.685068 ],\n",
              "       [22.606396 ],\n",
              "       [26.904682 ],\n",
              "       [36.55852  ],\n",
              "       [25.189812 ],\n",
              "       [24.660734 ],\n",
              "       [26.39839  ],\n",
              "       [31.108147 ],\n",
              "       [20.979042 ],\n",
              "       [31.836666 ],\n",
              "       [17.673615 ],\n",
              "       [24.810629 ],\n",
              "       [19.689049 ],\n",
              "       [37.84256  ],\n",
              "       [27.345478 ],\n",
              "       [16.248917 ],\n",
              "       [27.789757 ],\n",
              "       [19.697437 ],\n",
              "       [28.170296 ],\n",
              "       [16.79195  ],\n",
              "       [35.679306 ],\n",
              "       [36.422993 ],\n",
              "       [11.021306 ],\n",
              "       [38.121662 ],\n",
              "       [33.28984  ],\n",
              "       [32.7008   ],\n",
              "       [29.568008 ],\n",
              "       [28.646269 ],\n",
              "       [37.13     ],\n",
              "       [31.71015  ],\n",
              "       [29.008512 ],\n",
              "       [27.08472  ],\n",
              "       [42.468693 ],\n",
              "       [26.202415 ],\n",
              "       [25.241642 ],\n",
              "       [27.973753 ],\n",
              "       [32.17257  ],\n",
              "       [18.25536  ],\n",
              "       [23.30566  ],\n",
              "       [36.413246 ],\n",
              "       [20.854027 ],\n",
              "       [32.43075  ],\n",
              "       [20.33839  ],\n",
              "       [20.948507 ],\n",
              "       [18.597605 ],\n",
              "       [33.308315 ],\n",
              "       [13.645658 ],\n",
              "       [27.861506 ],\n",
              "       [24.12751  ],\n",
              "       [26.039444 ],\n",
              "       [20.773438 ],\n",
              "       [21.389566 ],\n",
              "       [23.90348  ],\n",
              "       [36.593006 ],\n",
              "       [ 6.9346185],\n",
              "       [28.29148  ],\n",
              "       [33.464787 ],\n",
              "       [23.78146  ],\n",
              "       [26.134638 ],\n",
              "       [32.75471  ],\n",
              "       [10.753957 ],\n",
              "       [31.640968 ],\n",
              "       [26.263582 ],\n",
              "       [24.633833 ],\n",
              "       [26.71153  ],\n",
              "       [41.168285 ],\n",
              "       [32.389347 ],\n",
              "       [30.923985 ],\n",
              "       [31.973225 ],\n",
              "       [24.263298 ],\n",
              "       [28.575989 ],\n",
              "       [31.420326 ],\n",
              "       [28.929985 ],\n",
              "       [22.904957 ],\n",
              "       [36.0909   ],\n",
              "       [11.994477 ],\n",
              "       [33.794056 ],\n",
              "       [14.298036 ],\n",
              "       [26.31617  ],\n",
              "       [26.89558  ],\n",
              "       [32.351894 ],\n",
              "       [14.329368 ],\n",
              "       [24.2306   ],\n",
              "       [23.432812 ],\n",
              "       [25.909403 ],\n",
              "       [24.547005 ],\n",
              "       [24.026365 ],\n",
              "       [18.466045 ],\n",
              "       [20.595856 ],\n",
              "       [27.149883 ],\n",
              "       [32.8075   ],\n",
              "       [17.098888 ],\n",
              "       [38.833813 ],\n",
              "       [19.978807 ],\n",
              "       [18.686209 ],\n",
              "       [23.403767 ],\n",
              "       [11.146699 ],\n",
              "       [24.959969 ],\n",
              "       [24.671766 ],\n",
              "       [41.665386 ],\n",
              "       [27.294514 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측값과 실제값 비교\n",
        "y_pred = model.predict(x_test).flatten()\n",
        "y_pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvW5Ka2Ozd6Q",
        "outputId": "0f6dd563-b394-4297-ad89-c3b87c6d2cfb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([25.855886 , 13.758203 , 22.148754 , 19.378578 , 30.665382 ,\n",
              "       38.675354 , 27.451576 , 16.445278 , 14.68857  , 34.373184 ,\n",
              "       35.73653  , 25.104418 , 30.234303 , 30.945295 , 19.967203 ,\n",
              "       30.364902 , 24.677885 , 23.374546 , 22.192947 , 36.818386 ,\n",
              "       21.346704 , 13.232263 , 21.20731  , 20.163645 , 27.855843 ,\n",
              "       30.907291 , 18.372423 , 35.17461  , 20.851646 , 37.17119  ,\n",
              "       29.816927 , 17.058245 , 18.290375 , 17.927189 , 24.93911  ,\n",
              "       14.475602 , 25.680397 , 19.178524 , 30.780167 , 13.193921 ,\n",
              "       42.16373  , 28.398947 , 22.952267 , 24.57362  , 14.581895 ,\n",
              "       20.039904 , 26.60867  , 30.541538 , 23.104023 , 29.032475 ,\n",
              "       30.646107 , 26.685068 , 22.606396 , 26.904682 , 36.55852  ,\n",
              "       25.189812 , 24.660734 , 26.39839  , 31.108147 , 20.979042 ,\n",
              "       31.836666 , 17.673615 , 24.810629 , 19.689049 , 37.84256  ,\n",
              "       27.345478 , 16.248917 , 27.789757 , 19.697437 , 28.170296 ,\n",
              "       16.79195  , 35.679306 , 36.422993 , 11.021306 , 38.121662 ,\n",
              "       33.28984  , 32.7008   , 29.568008 , 28.646269 , 37.13     ,\n",
              "       31.71015  , 29.008512 , 27.08472  , 42.468693 , 26.202415 ,\n",
              "       25.241642 , 27.973753 , 32.17257  , 18.25536  , 23.30566  ,\n",
              "       36.413246 , 20.854027 , 32.43075  , 20.33839  , 20.948507 ,\n",
              "       18.597605 , 33.308315 , 13.645658 , 27.861506 , 24.12751  ,\n",
              "       26.039444 , 20.773438 , 21.389566 , 23.90348  , 36.593006 ,\n",
              "        6.9346185, 28.29148  , 33.464787 , 23.78146  , 26.134638 ,\n",
              "       32.75471  , 10.753957 , 31.640968 , 26.263582 , 24.633833 ,\n",
              "       26.71153  , 41.168285 , 32.389347 , 30.923985 , 31.973225 ,\n",
              "       24.263298 , 28.575989 , 31.420326 , 28.929985 , 22.904957 ,\n",
              "       36.0909   , 11.994477 , 33.794056 , 14.298036 , 26.31617  ,\n",
              "       26.89558  , 32.351894 , 14.329368 , 24.2306   , 23.432812 ,\n",
              "       25.909403 , 24.547005 , 24.026365 , 18.466045 , 20.595856 ,\n",
              "       27.149883 , 32.8075   , 17.098888 , 38.833813 , 19.978807 ,\n",
              "       18.686209 , 23.403767 , 11.146699 , 24.959969 , 24.671766 ,\n",
              "       41.665386 , 27.294514 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    label = y_test[i]\n",
        "    prediction = y_pred[i]\n",
        "\n",
        "    print(\"실제 가격 : {:.3f}, 예상가격 : {:.3f}\".format(label, prediction))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-OiuZahz2SA",
        "outputId": "57b1a0fd-840b-4ff6-ccc1-8d5aeaf9a9fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "실제 가격 : 18.900, 예상가격 : 25.856\n",
            "실제 가격 : 10.400, 예상가격 : 13.758\n",
            "실제 가격 : 27.500, 예상가격 : 22.149\n",
            "실제 가격 : 14.300, 예상가격 : 19.379\n",
            "실제 가격 : 25.000, 예상가격 : 30.665\n",
            "실제 가격 : 36.000, 예상가격 : 38.675\n",
            "실제 가격 : 21.200, 예상가격 : 27.452\n",
            "실제 가격 : 14.100, 예상가격 : 16.445\n",
            "실제 가격 : 17.300, 예상가격 : 14.689\n",
            "실제 가격 : 32.500, 예상가격 : 34.373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ".flatten() : 다차원 배열의 값을 일차원으로"
      ],
      "metadata": {
        "id": "bXBc7CkSzTbv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 분류 vs 회귀\n",
        "- 분류는 정확도를 찾는다.\n",
        "- 회귀는 근사값을 찾는다."
      ],
      "metadata": {
        "id": "qonmNkEl0YQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 나쁜 예시\n",
        "model.compile( loss='mean_squared_error', optimizer='adam', metrics=['accuracy']) # 평가기준 정확도(accuracy)가 0.0이 나온다.\n",
        "\n",
        "model.fit(x_train, y_train, epochs=200, batch_size=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IthZgRTvzAuv",
        "outputId": "0c5957c6-af12-447e-a0e9-542064af83f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "36/36 [==============================] - 1s 4ms/step - loss: 30.7152 - accuracy: 0.0000e+00\n",
            "Epoch 2/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 25.5167 - accuracy: 0.0000e+00\n",
            "Epoch 3/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 26.6628 - accuracy: 0.0000e+00\n",
            "Epoch 4/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.4416 - accuracy: 0.0000e+00\n",
            "Epoch 5/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.8455 - accuracy: 0.0000e+00\n",
            "Epoch 6/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.4333 - accuracy: 0.0000e+00\n",
            "Epoch 7/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.3648 - accuracy: 0.0000e+00\n",
            "Epoch 8/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.4729 - accuracy: 0.0000e+00\n",
            "Epoch 9/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 29.1622 - accuracy: 0.0000e+00\n",
            "Epoch 10/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 31.6950 - accuracy: 0.0000e+00\n",
            "Epoch 11/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.7434 - accuracy: 0.0000e+00\n",
            "Epoch 12/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.5748 - accuracy: 0.0000e+00\n",
            "Epoch 13/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.0493 - accuracy: 0.0000e+00\n",
            "Epoch 14/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.2082 - accuracy: 0.0000e+00\n",
            "Epoch 15/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.1822 - accuracy: 0.0000e+00\n",
            "Epoch 16/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.0283 - accuracy: 0.0000e+00\n",
            "Epoch 17/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.7456 - accuracy: 0.0000e+00\n",
            "Epoch 18/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.0357 - accuracy: 0.0000e+00\n",
            "Epoch 19/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 31.4692 - accuracy: 0.0000e+00\n",
            "Epoch 20/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.2788 - accuracy: 0.0000e+00\n",
            "Epoch 21/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 29.5602 - accuracy: 0.0000e+00\n",
            "Epoch 22/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.2309 - accuracy: 0.0000e+00\n",
            "Epoch 23/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.7754 - accuracy: 0.0000e+00\n",
            "Epoch 24/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.7811 - accuracy: 0.0000e+00\n",
            "Epoch 25/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.9439 - accuracy: 0.0000e+00\n",
            "Epoch 26/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 33.4722 - accuracy: 0.0000e+00\n",
            "Epoch 27/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.3956 - accuracy: 0.0000e+00\n",
            "Epoch 28/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.4589 - accuracy: 0.0000e+00\n",
            "Epoch 29/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 29.7326 - accuracy: 0.0000e+00\n",
            "Epoch 30/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.6958 - accuracy: 0.0000e+00\n",
            "Epoch 31/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.5390 - accuracy: 0.0000e+00\n",
            "Epoch 32/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.0078 - accuracy: 0.0000e+00\n",
            "Epoch 33/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.8320 - accuracy: 0.0000e+00\n",
            "Epoch 34/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.9846 - accuracy: 0.0000e+00\n",
            "Epoch 35/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.3573 - accuracy: 0.0000e+00\n",
            "Epoch 36/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.6217 - accuracy: 0.0000e+00\n",
            "Epoch 37/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.9754 - accuracy: 0.0000e+00\n",
            "Epoch 38/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.8389 - accuracy: 0.0000e+00\n",
            "Epoch 39/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.1258 - accuracy: 0.0000e+00\n",
            "Epoch 40/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.8917 - accuracy: 0.0000e+00\n",
            "Epoch 41/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.7696 - accuracy: 0.0000e+00\n",
            "Epoch 42/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.7541 - accuracy: 0.0000e+00\n",
            "Epoch 43/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 29.7134 - accuracy: 0.0000e+00\n",
            "Epoch 44/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.4953 - accuracy: 0.0000e+00\n",
            "Epoch 45/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.5901 - accuracy: 0.0000e+00\n",
            "Epoch 46/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 30.8371 - accuracy: 0.0000e+00\n",
            "Epoch 47/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 29.0427 - accuracy: 0.0000e+00\n",
            "Epoch 48/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.4443 - accuracy: 0.0000e+00\n",
            "Epoch 49/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.8125 - accuracy: 0.0000e+00\n",
            "Epoch 50/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 31.2477 - accuracy: 0.0000e+00\n",
            "Epoch 51/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.8770 - accuracy: 0.0000e+00\n",
            "Epoch 52/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 29.4635 - accuracy: 0.0000e+00\n",
            "Epoch 53/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.1744 - accuracy: 0.0000e+00\n",
            "Epoch 54/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.3124 - accuracy: 0.0000e+00\n",
            "Epoch 55/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.2550 - accuracy: 0.0000e+00\n",
            "Epoch 56/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.9627 - accuracy: 0.0000e+00\n",
            "Epoch 57/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.2865 - accuracy: 0.0000e+00\n",
            "Epoch 58/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.5724 - accuracy: 0.0000e+00\n",
            "Epoch 59/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.3246 - accuracy: 0.0000e+00\n",
            "Epoch 60/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.2857 - accuracy: 0.0000e+00\n",
            "Epoch 61/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.7138 - accuracy: 0.0000e+00\n",
            "Epoch 62/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.1344 - accuracy: 0.0000e+00\n",
            "Epoch 63/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.6031 - accuracy: 0.0000e+00\n",
            "Epoch 64/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.2552 - accuracy: 0.0000e+00\n",
            "Epoch 65/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.2895 - accuracy: 0.0000e+00\n",
            "Epoch 66/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.0963 - accuracy: 0.0000e+00\n",
            "Epoch 67/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.2123 - accuracy: 0.0000e+00\n",
            "Epoch 68/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.1810 - accuracy: 0.0000e+00\n",
            "Epoch 69/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.7828 - accuracy: 0.0000e+00\n",
            "Epoch 70/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.1812 - accuracy: 0.0000e+00\n",
            "Epoch 71/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.9193 - accuracy: 0.0000e+00\n",
            "Epoch 72/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.4917 - accuracy: 0.0000e+00\n",
            "Epoch 73/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.9130 - accuracy: 0.0000e+00\n",
            "Epoch 74/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.4933 - accuracy: 0.0000e+00\n",
            "Epoch 75/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.1701 - accuracy: 0.0000e+00\n",
            "Epoch 76/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.4541 - accuracy: 0.0000e+00\n",
            "Epoch 77/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.4923 - accuracy: 0.0000e+00\n",
            "Epoch 78/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.5956 - accuracy: 0.0000e+00\n",
            "Epoch 79/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.5642 - accuracy: 0.0000e+00\n",
            "Epoch 80/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.1721 - accuracy: 0.0000e+00\n",
            "Epoch 81/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.8246 - accuracy: 0.0000e+00\n",
            "Epoch 82/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.3532 - accuracy: 0.0000e+00\n",
            "Epoch 83/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.1699 - accuracy: 0.0000e+00\n",
            "Epoch 84/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.7281 - accuracy: 0.0000e+00\n",
            "Epoch 85/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.1477 - accuracy: 0.0000e+00\n",
            "Epoch 86/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.5408 - accuracy: 0.0000e+00\n",
            "Epoch 87/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.4100 - accuracy: 0.0000e+00\n",
            "Epoch 88/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.6547 - accuracy: 0.0000e+00\n",
            "Epoch 89/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.7536 - accuracy: 0.0000e+00\n",
            "Epoch 90/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.3737 - accuracy: 0.0000e+00\n",
            "Epoch 91/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.7356 - accuracy: 0.0000e+00\n",
            "Epoch 92/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.8416 - accuracy: 0.0000e+00\n",
            "Epoch 93/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.3541 - accuracy: 0.0000e+00\n",
            "Epoch 94/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 29.1482 - accuracy: 0.0000e+00\n",
            "Epoch 95/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.6226 - accuracy: 0.0000e+00\n",
            "Epoch 96/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.1569 - accuracy: 0.0000e+00\n",
            "Epoch 97/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.4925 - accuracy: 0.0000e+00\n",
            "Epoch 98/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.9680 - accuracy: 0.0000e+00\n",
            "Epoch 99/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.0425 - accuracy: 0.0000e+00\n",
            "Epoch 100/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 29.9781 - accuracy: 0.0000e+00\n",
            "Epoch 101/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.8035 - accuracy: 0.0000e+00\n",
            "Epoch 102/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.4493 - accuracy: 0.0000e+00\n",
            "Epoch 103/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.1986 - accuracy: 0.0000e+00\n",
            "Epoch 104/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.7173 - accuracy: 0.0000e+00\n",
            "Epoch 105/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.7592 - accuracy: 0.0000e+00\n",
            "Epoch 106/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.3840 - accuracy: 0.0000e+00\n",
            "Epoch 107/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.6754 - accuracy: 0.0000e+00\n",
            "Epoch 108/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.8152 - accuracy: 0.0000e+00\n",
            "Epoch 109/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.2003 - accuracy: 0.0000e+00\n",
            "Epoch 110/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.4253 - accuracy: 0.0000e+00\n",
            "Epoch 111/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.9044 - accuracy: 0.0000e+00\n",
            "Epoch 112/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.0378 - accuracy: 0.0000e+00\n",
            "Epoch 113/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.2329 - accuracy: 0.0000e+00\n",
            "Epoch 114/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.8977 - accuracy: 0.0000e+00\n",
            "Epoch 115/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.5970 - accuracy: 0.0000e+00\n",
            "Epoch 116/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.2813 - accuracy: 0.0000e+00\n",
            "Epoch 117/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 24.4178 - accuracy: 0.0000e+00\n",
            "Epoch 118/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 25.7877 - accuracy: 0.0000e+00\n",
            "Epoch 119/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 29.2309 - accuracy: 0.0000e+00\n",
            "Epoch 120/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 28.5032 - accuracy: 0.0000e+00\n",
            "Epoch 121/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 25.3076 - accuracy: 0.0000e+00\n",
            "Epoch 122/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 24.9420 - accuracy: 0.0000e+00\n",
            "Epoch 123/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 29.6415 - accuracy: 0.0000e+00\n",
            "Epoch 124/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 24.7337 - accuracy: 0.0000e+00\n",
            "Epoch 125/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 24.0328 - accuracy: 0.0000e+00\n",
            "Epoch 126/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 22.7553 - accuracy: 0.0000e+00\n",
            "Epoch 127/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 25.6502 - accuracy: 0.0000e+00\n",
            "Epoch 128/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 22.8400 - accuracy: 0.0000e+00\n",
            "Epoch 129/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 22.8343 - accuracy: 0.0000e+00\n",
            "Epoch 130/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.6742 - accuracy: 0.0000e+00\n",
            "Epoch 131/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.2954 - accuracy: 0.0000e+00\n",
            "Epoch 132/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.8910 - accuracy: 0.0000e+00\n",
            "Epoch 133/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.4439 - accuracy: 0.0000e+00\n",
            "Epoch 134/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.7265 - accuracy: 0.0000e+00\n",
            "Epoch 135/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.2581 - accuracy: 0.0000e+00\n",
            "Epoch 136/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.0521 - accuracy: 0.0000e+00\n",
            "Epoch 137/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.2516 - accuracy: 0.0000e+00\n",
            "Epoch 138/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.3994 - accuracy: 0.0000e+00\n",
            "Epoch 139/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.6988 - accuracy: 0.0000e+00\n",
            "Epoch 140/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.8688 - accuracy: 0.0000e+00\n",
            "Epoch 141/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.3909 - accuracy: 0.0000e+00\n",
            "Epoch 142/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.8321 - accuracy: 0.0000e+00\n",
            "Epoch 143/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.2322 - accuracy: 0.0000e+00\n",
            "Epoch 144/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 26.2712 - accuracy: 0.0000e+00\n",
            "Epoch 145/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.6005 - accuracy: 0.0000e+00\n",
            "Epoch 146/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.3239 - accuracy: 0.0000e+00\n",
            "Epoch 147/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.7534 - accuracy: 0.0000e+00\n",
            "Epoch 148/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 25.0878 - accuracy: 0.0000e+00\n",
            "Epoch 149/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.6483 - accuracy: 0.0000e+00\n",
            "Epoch 150/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 23.6717 - accuracy: 0.0000e+00\n",
            "Epoch 151/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 23.5237 - accuracy: 0.0000e+00\n",
            "Epoch 152/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 28.6353 - accuracy: 0.0000e+00\n",
            "Epoch 153/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.2410 - accuracy: 0.0000e+00\n",
            "Epoch 154/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.3406 - accuracy: 0.0000e+00\n",
            "Epoch 155/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.1502 - accuracy: 0.0000e+00\n",
            "Epoch 156/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 25.1152 - accuracy: 0.0000e+00\n",
            "Epoch 157/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 23.1645 - accuracy: 0.0000e+00\n",
            "Epoch 158/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 22.8106 - accuracy: 0.0000e+00\n",
            "Epoch 159/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 24.2141 - accuracy: 0.0000e+00\n",
            "Epoch 160/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 23.4788 - accuracy: 0.0000e+00\n",
            "Epoch 161/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 21.9735 - accuracy: 0.0000e+00\n",
            "Epoch 162/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 22.1387 - accuracy: 0.0000e+00\n",
            "Epoch 163/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 22.8545 - accuracy: 0.0000e+00\n",
            "Epoch 164/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 22.4701 - accuracy: 0.0000e+00\n",
            "Epoch 165/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.6227 - accuracy: 0.0000e+00\n",
            "Epoch 166/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.7123 - accuracy: 0.0000e+00\n",
            "Epoch 167/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.1882 - accuracy: 0.0000e+00\n",
            "Epoch 168/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.3370 - accuracy: 0.0000e+00\n",
            "Epoch 169/200\n",
            "26/36 [====================>.........] - ETA: 0s - loss: 17.8770 - accuracy: 0.0000e+00"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-222a5601ea99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 정확도가 0.0이 나온다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1187\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    433\u001b[0m     \"\"\"\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    293\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized hook: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    313\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1099\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1101\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_finalize_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m       \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m       \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m                     \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                     \u001b[0;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 좋은 예시\n",
        "model.compile( loss='mean_squared_error', optimizer='adam', metrics=['MSE']) # MSE : 평가기준 근사치로 해야한다.\n",
        "\n",
        "model.fit(x_train, y_train, epochs=200, batch_size=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJOvWlAMytTw",
        "outputId": "db02f1ed-fba1-4431-b6ce-8b1f5799dc40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "36/36 [==============================] - 1s 4ms/step - loss: 26.6991 - MSE: 26.6991\n",
            "Epoch 2/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 24.6200 - MSE: 24.6200\n",
            "Epoch 3/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 24.7026 - MSE: 24.7026\n",
            "Epoch 4/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.4229 - MSE: 22.4229\n",
            "Epoch 5/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.9612 - MSE: 27.9612\n",
            "Epoch 6/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.5167 - MSE: 22.5167\n",
            "Epoch 7/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.3303 - MSE: 23.3303\n",
            "Epoch 8/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.8330 - MSE: 22.8330\n",
            "Epoch 9/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.5600 - MSE: 24.5600\n",
            "Epoch 10/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.1152 - MSE: 22.1152\n",
            "Epoch 11/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.8292 - MSE: 21.8292\n",
            "Epoch 12/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.8227 - MSE: 22.8227\n",
            "Epoch 13/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.1157 - MSE: 23.1157\n",
            "Epoch 14/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.1782 - MSE: 27.1782\n",
            "Epoch 15/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.8375 - MSE: 21.8375\n",
            "Epoch 16/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 23.4356 - MSE: 23.4356\n",
            "Epoch 17/200\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 24.4626 - MSE: 24.4626\n",
            "Epoch 18/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 21.6237 - MSE: 21.6237\n",
            "Epoch 19/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 24.3719 - MSE: 24.3719\n",
            "Epoch 20/200\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 23.7467 - MSE: 23.7467\n",
            "Epoch 21/200\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 23.8145 - MSE: 23.8145\n",
            "Epoch 22/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 22.4442 - MSE: 22.4442\n",
            "Epoch 23/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 22.2689 - MSE: 22.2689\n",
            "Epoch 24/200\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 25.5691 - MSE: 25.5691\n",
            "Epoch 25/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 23.2907 - MSE: 23.2907\n",
            "Epoch 26/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 25.4187 - MSE: 25.4187\n",
            "Epoch 27/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 26.3578 - MSE: 26.3578\n",
            "Epoch 28/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 22.1041 - MSE: 22.1041\n",
            "Epoch 29/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 22.1949 - MSE: 22.1949\n",
            "Epoch 30/200\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 25.2090 - MSE: 25.2090\n",
            "Epoch 31/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 23.6089 - MSE: 23.6089\n",
            "Epoch 32/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 23.1753 - MSE: 23.1753\n",
            "Epoch 33/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.3779 - MSE: 23.3779\n",
            "Epoch 34/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.1726 - MSE: 23.1726\n",
            "Epoch 35/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.3076 - MSE: 23.3076\n",
            "Epoch 36/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.6825 - MSE: 23.6825\n",
            "Epoch 37/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.3021 - MSE: 22.3021\n",
            "Epoch 38/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.5142 - MSE: 22.5142\n",
            "Epoch 39/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.7047 - MSE: 21.7047\n",
            "Epoch 40/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.1147 - MSE: 22.1147\n",
            "Epoch 41/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.4945 - MSE: 27.4945\n",
            "Epoch 42/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.8119 - MSE: 24.8119\n",
            "Epoch 43/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.5352 - MSE: 23.5352\n",
            "Epoch 44/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 20.8587 - MSE: 20.8587\n",
            "Epoch 45/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.6882 - MSE: 23.6882\n",
            "Epoch 46/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.4056 - MSE: 27.4056\n",
            "Epoch 47/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 28.2829 - MSE: 28.2829\n",
            "Epoch 48/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 26.0745 - MSE: 26.0745\n",
            "Epoch 49/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.7102 - MSE: 23.7102\n",
            "Epoch 50/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.2566 - MSE: 24.2566\n",
            "Epoch 51/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.4317 - MSE: 23.4317\n",
            "Epoch 52/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.1400 - MSE: 22.1400\n",
            "Epoch 53/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.0542 - MSE: 21.0542\n",
            "Epoch 54/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.1407 - MSE: 21.1407\n",
            "Epoch 55/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 20.9747 - MSE: 20.9747\n",
            "Epoch 56/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.5864 - MSE: 21.5864\n",
            "Epoch 57/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.6032 - MSE: 22.6032\n",
            "Epoch 58/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.0623 - MSE: 23.0623\n",
            "Epoch 59/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.8579 - MSE: 22.8579\n",
            "Epoch 60/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.4336 - MSE: 24.4336\n",
            "Epoch 61/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.1685 - MSE: 24.1685\n",
            "Epoch 62/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.1184 - MSE: 24.1184\n",
            "Epoch 63/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.5592 - MSE: 21.5592\n",
            "Epoch 64/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.1477 - MSE: 24.1477\n",
            "Epoch 65/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 20.9337 - MSE: 20.9337\n",
            "Epoch 66/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.4921 - MSE: 22.4921\n",
            "Epoch 67/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.1549 - MSE: 27.1549\n",
            "Epoch 68/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.5207 - MSE: 21.5207\n",
            "Epoch 69/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.3565 - MSE: 23.3565\n",
            "Epoch 70/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.4519 - MSE: 22.4519\n",
            "Epoch 71/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.7751 - MSE: 21.7751\n",
            "Epoch 72/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.4755 - MSE: 23.4755\n",
            "Epoch 73/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.1290 - MSE: 24.1290\n",
            "Epoch 74/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.7091 - MSE: 21.7091\n",
            "Epoch 75/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.4248 - MSE: 21.4248\n",
            "Epoch 76/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.9315 - MSE: 23.9315\n",
            "Epoch 77/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.3132 - MSE: 24.3132\n",
            "Epoch 78/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.2509 - MSE: 21.2509\n",
            "Epoch 79/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.4379 - MSE: 21.4379\n",
            "Epoch 80/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.3258 - MSE: 21.3258\n",
            "Epoch 81/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.9523 - MSE: 24.9523\n",
            "Epoch 82/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.1404 - MSE: 22.1404\n",
            "Epoch 83/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.4412 - MSE: 21.4412\n",
            "Epoch 84/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.9450 - MSE: 21.9450\n",
            "Epoch 85/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 20.9014 - MSE: 20.9014\n",
            "Epoch 86/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.4263 - MSE: 21.4263\n",
            "Epoch 87/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.4388 - MSE: 22.4388\n",
            "Epoch 88/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.5798 - MSE: 24.5798\n",
            "Epoch 89/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.7958 - MSE: 24.7958\n",
            "Epoch 90/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.7598 - MSE: 21.7598\n",
            "Epoch 91/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 20.7202 - MSE: 20.7202\n",
            "Epoch 92/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.4263 - MSE: 21.4263\n",
            "Epoch 93/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.7342 - MSE: 23.7342\n",
            "Epoch 94/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.1148 - MSE: 21.1148\n",
            "Epoch 95/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.5896 - MSE: 22.5896\n",
            "Epoch 96/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 20.3798 - MSE: 20.3798\n",
            "Epoch 97/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.7119 - MSE: 21.7119\n",
            "Epoch 98/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.3768 - MSE: 22.3768\n",
            "Epoch 99/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.4528 - MSE: 21.4528\n",
            "Epoch 100/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.7760 - MSE: 24.7760\n",
            "Epoch 101/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.1315 - MSE: 23.1315\n",
            "Epoch 102/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 20.6035 - MSE: 20.6035\n",
            "Epoch 103/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.5794 - MSE: 23.5794\n",
            "Epoch 104/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.3218 - MSE: 21.3218\n",
            "Epoch 105/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 20.5881 - MSE: 20.5881\n",
            "Epoch 106/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.3189 - MSE: 22.3189\n",
            "Epoch 107/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.7055 - MSE: 22.7055\n",
            "Epoch 108/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.3933 - MSE: 22.3933\n",
            "Epoch 109/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.7302 - MSE: 21.7302\n",
            "Epoch 110/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.2878 - MSE: 21.2878\n",
            "Epoch 111/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.2368 - MSE: 22.2368\n",
            "Epoch 112/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.2946 - MSE: 27.2946\n",
            "Epoch 113/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.3790 - MSE: 23.3790\n",
            "Epoch 114/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.9463 - MSE: 22.9463\n",
            "Epoch 115/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.6325 - MSE: 21.6325\n",
            "Epoch 116/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 20.3121 - MSE: 20.3121\n",
            "Epoch 117/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.7002 - MSE: 21.7002\n",
            "Epoch 118/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.4310 - MSE: 22.4310\n",
            "Epoch 119/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.0015 - MSE: 22.0015\n",
            "Epoch 120/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.1358 - MSE: 24.1358\n",
            "Epoch 121/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 27.5965 - MSE: 27.5965\n",
            "Epoch 122/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.7006 - MSE: 22.7006\n",
            "Epoch 123/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 20.9511 - MSE: 20.9511\n",
            "Epoch 124/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.1082 - MSE: 21.1082\n",
            "Epoch 125/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.1409 - MSE: 21.1409\n",
            "Epoch 126/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.9591 - MSE: 25.9591\n",
            "Epoch 127/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 30.1582 - MSE: 30.1582\n",
            "Epoch 128/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.6115 - MSE: 22.6115\n",
            "Epoch 129/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.6785 - MSE: 22.6785\n",
            "Epoch 130/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 25.5797 - MSE: 25.5797\n",
            "Epoch 131/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.0015 - MSE: 22.0015\n",
            "Epoch 132/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.6164 - MSE: 22.6164\n",
            "Epoch 133/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.4698 - MSE: 21.4698\n",
            "Epoch 134/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.3230 - MSE: 21.3230\n",
            "Epoch 135/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.1338 - MSE: 23.1338\n",
            "Epoch 136/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 20.4307 - MSE: 20.4307\n",
            "Epoch 137/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.6233 - MSE: 21.6233\n",
            "Epoch 138/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.3110 - MSE: 22.3110\n",
            "Epoch 139/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 21.0589 - MSE: 21.0589\n",
            "Epoch 140/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.5760 - MSE: 21.5760\n",
            "Epoch 141/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.7930 - MSE: 21.7930\n",
            "Epoch 142/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.5348 - MSE: 21.5348\n",
            "Epoch 143/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.3930 - MSE: 21.3930\n",
            "Epoch 144/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 20.8559 - MSE: 20.8559\n",
            "Epoch 145/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 20.2483 - MSE: 20.2483\n",
            "Epoch 146/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.5683 - MSE: 21.5683\n",
            "Epoch 147/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 22.2686 - MSE: 22.2686\n",
            "Epoch 148/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.4652 - MSE: 21.4652\n",
            "Epoch 149/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 20.2081 - MSE: 20.2081\n",
            "Epoch 150/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 19.8232 - MSE: 19.8232\n",
            "Epoch 151/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.8467 - MSE: 21.8467\n",
            "Epoch 152/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 20.1509 - MSE: 20.1509\n",
            "Epoch 153/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 20.8756 - MSE: 20.8756\n",
            "Epoch 154/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 20.1040 - MSE: 20.1040\n",
            "Epoch 155/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.9345 - MSE: 21.9345\n",
            "Epoch 156/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.0570 - MSE: 21.0570\n",
            "Epoch 157/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 19.9603 - MSE: 19.9603\n",
            "Epoch 158/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 19.6398 - MSE: 19.6398\n",
            "Epoch 159/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 20.2973 - MSE: 20.2973\n",
            "Epoch 160/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 20.1881 - MSE: 20.1881\n",
            "Epoch 161/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 22.6407 - MSE: 22.6407\n",
            "Epoch 162/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 20.1527 - MSE: 20.1527\n",
            "Epoch 163/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 21.0908 - MSE: 21.0908\n",
            "Epoch 164/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 19.5233 - MSE: 19.5233\n",
            "Epoch 165/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 20.6674 - MSE: 20.6674\n",
            "Epoch 166/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 20.8472 - MSE: 20.8472\n",
            "Epoch 167/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 21.2598 - MSE: 21.2598\n",
            "Epoch 168/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 24.6154 - MSE: 24.6154\n",
            "Epoch 169/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 19.8008 - MSE: 19.8008\n",
            "Epoch 170/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 20.3200 - MSE: 20.3200\n",
            "Epoch 171/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 20.0888 - MSE: 20.0888\n",
            "Epoch 172/200\n",
            "36/36 [==============================] - 0s 5ms/step - loss: 19.9270 - MSE: 19.9270\n",
            "Epoch 173/200\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 21.4926 - MSE: 21.4926\n",
            "Epoch 174/200\n",
            "36/36 [==============================] - 0s 7ms/step - loss: 20.9796 - MSE: 20.9796\n",
            "Epoch 175/200\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 21.3918 - MSE: 21.3918\n",
            "Epoch 176/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 19.9186 - MSE: 19.9186\n",
            "Epoch 177/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 22.6900 - MSE: 22.6900\n",
            "Epoch 178/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 20.0920 - MSE: 20.0920\n",
            "Epoch 179/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 32.2295 - MSE: 32.2295\n",
            "Epoch 180/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 24.9264 - MSE: 24.9264\n",
            "Epoch 181/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 19.5357 - MSE: 19.5357\n",
            "Epoch 182/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.5742 - MSE: 21.5742\n",
            "Epoch 183/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 20.5512 - MSE: 20.5512\n",
            "Epoch 184/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 23.8364 - MSE: 23.8364\n",
            "Epoch 185/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 20.1808 - MSE: 20.1808\n",
            "Epoch 186/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 20.1820 - MSE: 20.1820\n",
            "Epoch 187/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 19.6923 - MSE: 19.6923\n",
            "Epoch 188/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.7171 - MSE: 21.7171\n",
            "Epoch 189/200\n",
            "36/36 [==============================] - 0s 6ms/step - loss: 22.5535 - MSE: 22.5535\n",
            "Epoch 190/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 24.0280 - MSE: 24.0280\n",
            "Epoch 191/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 19.5045 - MSE: 19.5045\n",
            "Epoch 192/200\n",
            "36/36 [==============================] - 0s 3ms/step - loss: 22.7551 - MSE: 22.7551\n",
            "Epoch 193/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 20.5814 - MSE: 20.5814\n",
            "Epoch 194/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 20.3551 - MSE: 20.3551\n",
            "Epoch 195/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.1902 - MSE: 21.1902\n",
            "Epoch 196/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 19.6583 - MSE: 19.6583\n",
            "Epoch 197/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 20.9961 - MSE: 20.9961\n",
            "Epoch 198/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 20.0943 - MSE: 20.0943\n",
            "Epoch 199/200\n",
            "36/36 [==============================] - 0s 2ms/step - loss: 21.6346 - MSE: 21.6346\n",
            "Epoch 200/200\n",
            "36/36 [==============================] - 0s 4ms/step - loss: 19.6547 - MSE: 19.6547\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1d04b018d0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 가상화\n",
        "- 단일 컴퓨터의 하드웨어 요소를 가상머신이라는 다수의 가상 컴퓨터로 분할\n",
        "\n",
        "# 클라우드 컴퓨팅\n",
        "- 클라우드 컴퓨팅은 인터넷 기술을 이용하여 내 외부 고객들에게 확장성(Scaleable)있고 탄력적(Elastic)인 IT 서비스가 제공되는 방식\n",
        "\n",
        "# 완전 가상화 (Full Emulated)\n",
        "- 모든 시스템 요소가 에뮬레이터 안에서 돌아감\n",
        "- OS와 연동, 느림\n",
        "\n",
        "# 2세대\n",
        "- 하이퍼바이저 : OS와 하드웨어 사이에 존재하는 일종의 가상화 매니저\n",
        "\n",
        "# 3세대\n",
        "- 하드웨어 직접 가상화 지원\n",
        "\n",
        "# IaaS & PaaS & SaaS\n",
        "- Application => Data => Runtime => HW => OS => Virtualization => Servers => Storage => Network\n",
        "- IaaS : App ~ Network 지원\n",
        "- PaaS : Runtime ~ Network 지원\n",
        "- SaaS : OS ~ Network 지원\n",
        "\n",
        "# 클라우드 & 하이브리드 & 온프레미스\n",
        "- 클라우드 : AWS, GCP\n",
        "- 온프레미스 : VMWare\n",
        "\n",
        "# 클라우드 장점 \n",
        "- 자본 비용을 가변비용으로 대체\n",
        "- 규모의 경제로서 이점\n",
        "- 용량 추정 불필요\n",
        "- 속도 및 민첩성 개선\n",
        "- 중요한 문제에 집중\n",
        "- 몇 분 만에 전세계에 배포\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CBjCVe5W_AFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AWS\n",
        "- EC2 : 가상서버\n",
        "- EBS : 저장공간\n",
        "- S3 : 글로벌 저장공간 + Web 호스팅\n",
        "- ERS : 파일 단위의 저장공간\n",
        "- VPC : 가상 전용 (setting 직접함)\n",
        "-  Route 53 : 도메인\n",
        "- ELB : 트래픽 서버\n",
        "- Auto Scaleing : 서버 크기 조정\n",
        "- Code: IaC (코드 기반)\n",
        "- IAM : 사용자 계정 관리\n",
        "- Lambda : 서버리스\n",
        "- Migration : 서버이동"
      ],
      "metadata": {
        "id": "M0QsKE89LHRK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AWS 접근 방법\n",
        "- AWS 콘솔, CLI, SDK, CloudFormation (IaC : 템플릿 : JSON 기반 웹 API)"
      ],
      "metadata": {
        "id": "6S_H5LbzGwrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Amazon EC2\n",
        "- 타입과 서비스에 따라 적합한 사양을 선택가능\n",
        "\n",
        "# Amazon Auto Scaleing\n",
        "- 서버 사용량에 따라 추가 삭제를 가능하게 하는 서비스\n",
        "\n",
        "# Amazon Lightsail\n",
        "- 저렴한 비용\n",
        "\n",
        "# Amazon WorkSpaces\n",
        "- VDI (데스크탑 가상화 서비스)\n"
      ],
      "metadata": {
        "id": "Pp754qaPHkMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 네트워크 서비스\n",
        "\n",
        "# Amazon Route 53\n",
        "- DNS 웹서비스\n",
        "\n",
        "# Amazon VPC\n",
        "- 가상 사설 네트워크를 클라우드 내에 직접 생성\n",
        "\n",
        "# Amazon Direct Connect\n",
        "- 온프레미스 인프라와 AWS를 연결하는 전용선 구성\n",
        "- 속도 빠름\n",
        "\n",
        "# Amazon ELB\n",
        "- 접속자가 많으면 트래픽을 부하 분산\n",
        "\n"
      ],
      "metadata": {
        "id": "DNQ-V_XRJXjS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 스토리지 서비스\n",
        "\n",
        "# Amazon S3\n",
        "- 여러 용도로 사용하는 범용적인 스토리지 서비스\n",
        "- 글로벌 저장공간, 정적 웹호스팅\n",
        "\n",
        "# Amazon Glacier\n",
        "- 저렴한 비용, 장기 보관\n",
        "\n",
        "# Amazon EBS\n",
        "- 데이터 저장 보관\n",
        "- 디스크로 추가하여 데이터를 보관, 제공\n",
        "\n",
        "# Amazon Storage Gateway\n",
        "- 온프레미스에서 클라우드로 저장 보관 하는 게이트 웨이\n",
        "\n",
        "# Amazon Snowball\n",
        "- 대량의 데이터 AWS로 이전할 때 물리적 전달"
      ],
      "metadata": {
        "id": "wze4gLeJJZPC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터베이스 서비스\n",
        "\n",
        "# Amazon RDS\n",
        "- DB를 직접 관리하지 않고 Amazon 제공 서비스로 DB 운영\n",
        "\n",
        "# Amazon DynamoDB\n",
        "- NoSQL 용 데이터\n",
        "\n",
        "# Amazon ElastiCache\n",
        "- in-memory 기반 Cache 서비스"
      ],
      "metadata": {
        "id": "efRUZhTFKXv6"
      }
    }
  ]
}